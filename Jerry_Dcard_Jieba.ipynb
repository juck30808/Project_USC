{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Read data\n",
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#設定matplot可以直接在Notebook中畫出，不需要執行plt.plot()\n",
    "%matplotlib inline \n",
    "\n",
    "# 設定 Seaborn 的顯示樣式\n",
    "# https://seaborn.pydata.org/generated/seaborn.set_style.html\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "#載入資料\n",
    "#data = pd.read_csv(\"data/Op_Dcard_markeup.csv\")\n",
    "data = pd.read_csv(\"data/Dcard_makeup.csv\")\n",
    "data\n",
    "print(data.shape)  #type(pandas.core.frame.DataFrame)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) 格式轉換\n",
    "\n",
    "- transform Type(pandas.core.series.Series) to list\n",
    "- transform Type(list) to str\n",
    "- addition: https://blog.csdn.net/Zx_whu/article/details/61926655"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = data['content'].tolist()   # transform Type(pandas.core.series.Series) to list\n",
    "for i in range(len(dt_list)):\n",
    "    print(dt_list[i])\n",
    "    print('====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt_list[0]      #Content 第一篇文章\n",
    "#dt_list[1]     #Content 第二篇文章\n",
    "#dt_list[87]    #最後一篇文章 #type(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Jeiba 語句分析\n",
    "\n",
    "- https://github.com/ldkrsi/jieba-zh_TW\n",
    "- https://blog.kennycoder.io/2020/02/12/Python-知名Jieba中文斷詞工具教學/\n",
    "\n",
    "jieba.cut生成的是一个生成器，generator，也就是可以通过for循环来取里面的每一个词。\n",
    "- 全模式: 把句子中所有的可以成詞的詞語都斷出来\n",
    "- jieba.cut(sentence, cut_all=True)\n",
    "- 精確模式 : 將句子最精確的切開，適合文本分析\n",
    "- jieba.cut(sentence, cut_all=False)\n",
    "- 搜索引擎模式: 在精確模式的基礎上，對長的詞語再次切分，提高召回率\n",
    "- jieba.cut_for_search(sentence)\n",
    "\n",
    "jieba.lcut 直接生成的就是一个list\n",
    "- jieba.lcut(news)\n",
    "\n",
    "## 負面關鍵字過濾前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "jieba.load_userdict('data/usedict.txt')   #adjust\n",
    "\n",
    "#dt_list = data['content'].tolist()   # transform Type(pandas.core.series.Series) to list\n",
    "\n",
    "for i in range(1):\n",
    "    #print(dt_list[i])\n",
    "    seg_list = jieba.lcut(dt_list[i])\n",
    "    #data['content'][i] = seg_list\n",
    "    print(seg_list)\n",
    "    print('====================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 負面關鍵字過濾後"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopWords=[]\n",
    "remainderWords=[]\n",
    "jieba.load_userdict('data/usedict.txt')   #adjust\n",
    "\n",
    "with open('data/stopWords.txt','r',encoding='utf-8') as file:\n",
    "    for w in file.readlines():\n",
    "        w=w.strip()\n",
    "        stopWords.append(w)\n",
    "\n",
    "for i in range(len(dt_list)):\n",
    "    seg_list = jieba.lcut(dt_list[i],cut_all=False)\n",
    "    remainderWords = list(filter(lambda a: a not in stopWords and a != '\\n', seg_list))\n",
    "    data['content'][i] = remainderWords\n",
    "    print(remainderWords)\n",
    "    print('====================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data['content'][0]\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data['content'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) Jeiba 分佈分析\n",
    "- TF-IDF算法 https://zhuanlan.zhihu.com/p/95358646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = data['content'].tolist()\n",
    "dt_str= ''.join(map(str,dt_list))\n",
    "dt_str  #type string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = jieba.analyse.extract_tags(dt_str, topK=30, withWeight=True)\n",
    "for tag in tags:\n",
    "    print('tf-idf:', round(tag[1],5),'| word:', tag[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) apyori 關聯性分析\n",
    "- https://pypi.org/project/apyori/\n",
    "- https://chwang12341.medium.com/machine-learning-關聯分析-apriori演算法-詳細解說啤酒與尿布的背後原理-python實作-scikit-learn一步一步教學-76b7778f8f34\n",
    "\n",
    "- pip install apyori\n",
    "- pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['content'][0],data['content'][1],data['content'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt =  [['T-Shirt','Pants','Jeans','Jersy','Socks','Basketball','Bottle','Shorts'],\n",
    "#        ['T-Shirt','Jeans'],\n",
    "#        ['Jersy','Basketball','Socks','Bottle'],\n",
    "#        ['Jeans','Pants','Bottle'],\n",
    "#        ['Shorts','Basketball'],\n",
    "#        ['Shorts','Jersy'],\n",
    "#        ['T-Shirt'],\n",
    "#        ['Basketball','Jersy'],\n",
    "#       ]\n",
    "\n",
    "# dt =  [data['content'][0],data['content'][1]\n",
    "#       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "## Data 自行定義數據\n",
    "dt =  [data['content'][0],data['content'][1],data['content'][2],data['content'][3],data['content'][4],\n",
    "       data['content'][5],data['content'][6],data['content'][7],data['content'][8],data['content'][9]\n",
    "       \n",
    "      ] \n",
    "                    \n",
    "association_rules = apriori(dt, min_support=0.2,   # 總共200筆交易數據，香腸出現20筆, SUP為 50/200 = 1/4,\n",
    "                            min_confidence=0.2,    # 兩物品同時出現的條件機率，出現商品A的情況下，出現商品B的機率\n",
    "                            min_lift=2,            # 數據間的關聯性，正相關>1 : 不相關=1 : 負相關<1\n",
    "                            max_length=2)\n",
    "                    \n",
    "association_results = list(association_rules)\n",
    "#print(association_results)\n",
    "\n",
    "\n",
    "for product in association_results:\n",
    "    pair = product[0] \n",
    "    products = [x for x in pair]\n",
    "    #print(products)\n",
    "    print(\"Rule: \" + products[0] + \" → \" + products[1])\n",
    "    print(\"Support: \" + str(product[1]))\n",
    "    print(\"Lift: \" + str(product[2][0][3]))\n",
    "    print(\"==================================\")\n",
    "    #print(product) # ex. RelationRecord(items=frozenset({'Basketball', 'Socks'}), support=0.25, ordered_statistics=[OrderedStatistic(items_base=frozenset({'Basketball'}), items_add=frozenset({'Socks'}), confidence=0.5, lift=2.0), OrderedStatistic(items_base=frozenset({'Socks'}), items_add=frozenset({'Basketball'}), confidence=1.0, lift=2.0)])\n",
    "    #print(pair)    # ex. frozenset({'Basketball', 'Socks'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "## Data Defined\n",
    "dt = {'Transaction ID': [1,2,3,4,5,6,7,8],\n",
    "      'Items':[['T-Shirt','Pants','Jeans','Jersy','Socks','Basketball','Bottle','Shorts'],\n",
    "               ['T-Shirt','Jeans'],\n",
    "               ['Jersy','Basketball','Socks','Bottle'],\n",
    "               ['Jeans','Pants','Bottle'],\n",
    "               ['Shorts','Basketball'],\n",
    "               ['Shorts','Jersy'],\n",
    "               ['T-Shirt'],\n",
    "               ['Basketball','Jersy'],              ]}\n",
    "\n",
    "data = pd.DataFrame(dt)\n",
    "pd.options.display.max_colwidth = 100        # 讓 DataFrame 能呈現的寬度大一點\n",
    "data_id = data.drop('Items', 1)              # 轉成數值編碼，目前都是字串的組合\n",
    "data_items = data.Items.str.join(',')\n",
    "data_items = data_items.str.get_dummies(',') # 轉成數值\n",
    "data = data_id.join(data_items)              ## 接上Transaction ID\n",
    "\n",
    "## 計算支持度 Support\n",
    "Support_items = apriori(data[['T-Shirt','Pants','Jeans','Jersy','Socks','Basketball','Bottle','Shorts']], \n",
    "                        min_support=0.20, use_colnames = True)\n",
    "\n",
    "## 計算關聯規則 Association Rule\n",
    "Association_Rules = association_rules(Support_items, metric = 'lift', min_threshold=1)\n",
    "Association_Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
